{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №3 (Нейронные сети)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение\n",
    "\n",
    "В ходе данной лабораторной работе необходимо использовать библиотеку Keras. Данная библиотека предоставляет удобный API для создания и обучения нейронных сетей. Документацию по данной библиотеке можно найти по ссылке https://keras.io/\n",
    "\n",
    "Все наборы данных находятся в модуле `keras.datasets`. Информацию об используемых в лабораторной работе наборах данных можно найти по ссылке https://keras.io/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "\n",
    "Доказано, что двухслойная нейронная сеть может успешно представить неразрывный выпуклый n-мерный многоугольник, а трехслойная нейронная сеть теоритечески способна аппроксимировать любые поверхности.  \n",
    "К сожалению, универсальность нейронных сетей зачастую нивелируется сложностью правильной настройки её гиперпараметров и структуры. Лабораторная работа состоит в том, чтобы увидеть, как влияет выбор тех или иных гиперпараметров на поведение всей сети, и как понять, что гиперпараметр установлен неверно.\n",
    "\n",
    "Задание состоит из следующих этапов:\n",
    "1. Изучить базовый принцип работы нейронных сетей\n",
    "2. Загрузить набор данных согласно варианту\n",
    "3. Закодировать данные понятным для сети образом. Для задач классификации выходные признаки закодировать one-hot кодированием.\n",
    "4. Используя интерактивный интерфейс, показать на графиках проблему переобучения нейронной сети*. Для этого нужно установить большое число слоев и нейронов в каждом слое. Переобучение возникает тогда, когда точности на тренировочной и тестовой выборках сильно разнятся между собой.\n",
    "5. Используя интерактивный интерфейс, показать на графиках что происходит при неправильном выборе темпа обучения\n",
    "6. Проделать то же самое для коэффициентов регуляризации.\n",
    "7. Поигравшись с гиперпараметрами, найти, на Ваш взгляд, оптимальные параметры нейронной сети и зафиксировать точность итоговой модели. Точность оптимальной модели желательно должна быть не меньше 75%.\n",
    "\n",
    "* -- выполнять данный пункт только при наличии GPU, совместимого с Tensorflow, т.к. обучение сети до состояния переобучения может занять слишком большое количество времени. Если видеокарта несовместима с tensorflow, можно попробовать backend Theano.\n",
    "\n",
    "По умолчанию, tensorflow использует только CPU. Чтобы tensorflow использовал GPU, нужно (предварительно активировав окружение `activate ai`): \n",
    "1. Удалить обычный пакет tensorflow: `pip uninstall tensorflow`\n",
    "2. Установить пакет tensorflow-gpu, NVIDIA CUDA и NVIDIA cuDNN, в соответствии с [инструкцией](https://www.tensorflow.org/install/gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Варианты\n",
    "\n",
    "|Вариант|Набор данных|Команда|Способ кодирования|\n",
    "|-|-|-|-|\n",
    "|1|CIFAR10|`from keras.datasets import cifar10`|One-hot для выходных признаков. Входные представить как одномерный массив.|\n",
    "|2|CIFAR100|`from keras.datasets import cifar100`|One-hot для выходных признаков. Входные представить как одномерный массив.|\n",
    "|3|MNIST Digits|`from keras.datasets import mnist`|One-hot для выходных признаков. Входные представить как одномерный массив.|\n",
    "|4|MNIST Fashion|`from keras.datasets import fashion_mnist`|One-hot для выходных признаков. Входные представить как одномерный массив.|\n",
    "|5|Boston Housing|`from keras.datasets import boston_housing`|Кодирование не требуется (задача регрессии)|\n",
    "|6*|IMDB|`from keras.datasets import imdb`|Выходные признаки оставить как есть. Входные -- Bag of Words (см. предыдущую л. р.)|\n",
    "|7*|IMDB|`from keras.datasets import imdb`|Выходные признаки оставить как есть. Входные -- TF-IDF (см. предыдущую л. р.)\n",
    "\n",
    "Варианты 6 и 7 объективно сложнее остальных и выбираются на усмотрение самими студентами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Содержание отчета\n",
    "\n",
    "1. Титульный лист\n",
    "2. Текст задания согласно варианту\n",
    "3. Исходный код\n",
    "4. Графики и пояснения к ним\n",
    "5. График обучения оптимальной модели\n",
    "6. Вывод по работе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вопросы к защите\n",
    "\n",
    "1. Устройство одиночного нейрона (персептрон)\n",
    "2. Сеть нейронов\n",
    "3. Метод обратного распространения ошибки\n",
    "4. Переобучение, паралич сети.\n",
    "5. Функции активации\n",
    "6. Виды регуляризации (L1, L2, ElasticNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инструкция"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a9fff4c55664>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input shape:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('Input shape:', x_train.shape)\n",
    "print('Output shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодирование признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input shape: (60000, 784)\n",
      "Encoded output shape: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot(y):\n",
    "    t = np.zeros((len(y), np.max(y) + 1))\n",
    "    t[np.arange(len(y)), y] = 1\n",
    "    return t\n",
    "\n",
    "y_train, y_test = one_hot(y_train), one_hot(y_test)\n",
    "x_train, x_test = x_train.reshape((x_train.shape[0], -1)), x_test.reshape((x_test.shape[0], -1))\n",
    "\n",
    "print('Encoded input shape:', x_train.shape)\n",
    "print('Encoded output shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерактивный функционал\n",
    "\n",
    "Для достижения оптимальной точности модели следует пользоваться следующими правилами:\n",
    "* Если решается задача регрессии, то последний слой должен иметь линейную активационную функцию\n",
    "* Если решается задача классификации, то последний слой может иметь активационную функцию softmax, sigmoid, tanh. softmax предпочтительна.\n",
    "* Скрытые слои обычно имеют следующие активационные функции: relu, sigmoid, tanh. Обычно берется relu, затем, если точность не устраивает, можно попробовать две другие.\n",
    "* Функция потерь выбирается следующим образом:\n",
    "    * Если решается задача классификации:\n",
    "        * Если классов ровно два, можно применять бинарную энтропию\n",
    "        * Если больше, то применяется категориальная энтропия\n",
    "        * Другие функции применять можно, но эффективность обучения резко снизится\n",
    "    * Если решается задача регрессии, то берется С.К.О. или среднее абсолютное отклонение.\n",
    "    \n",
    "    \n",
    "Для построения модели и графиков можно использовать скрипт, написанный ниже. Однако, при надобности, не запрещается писать собственный скрипт."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c835adb7f1d345549fbe9403af9fae91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='Скрытых слоев: ', max=3, style=SliderStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from ipywidgets import *\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.callbacks import Callback\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "\n",
    "def plot_accuracy(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history['acc'])\n",
    "    plt.plot(history['val_acc'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model = None\n",
    "history = {'acc': [], 'val_acc': [], 'loss': [], 'val_loss': []}\n",
    "style = {'description_width': 'initial'}\n",
    "@interact_manual(\n",
    "    n_layers=IntSlider(min=0, max=3, value=1, description='Скрытых слоев: ', style=style),\n",
    "    layer1_size=IntSlider(min=0, max=20, value=6, description='Нейронов в слое 1: ', style=style),\n",
    "    layer2_size=IntSlider(min=0, max=20, value=3, description='Нейронов в слое 2: ', style=style),\n",
    "    layer3_size=IntSlider(min=0, max=20, value=0, description='Нейронов в слое 3: ', style=style),\n",
    "    inner_layers_type=Dropdown(options=['softmax', 'relu', 'tanh', 'sigmoid', 'linear'], value='relu',\n",
    "                          description='Активация скрытых слоев: ', style=style),\n",
    "    layer_out_type=Dropdown(options=['softmax', 'relu', 'tanh', 'sigmoid', 'linear'], value='sigmoid',\n",
    "                          description='Активация выходного слоя: ', style=style),\n",
    "    loss_func=Dropdown(options={\n",
    "        'Среднеквадратическое отклонение': 'mse', \n",
    "        'Среднее абсолютное отклонение': 'mae',\n",
    "        'Бинарная энтропия': 'binary_crossentropy',\n",
    "        'Категориальная энтропия': 'categorical_crossentropy'\n",
    "    }, value='categorical_crossentropy', description='Функция потерь: ', style=style),\n",
    "    lr=ToggleButtons(options=[\"-0.1\", \"0\", \"0.001\", \"0.01\", \"0.05\", \"0.1\", \"0.5\", \"1\", \"5\"], \n",
    "                               value=\"0.01\", description='Темп обучения: ', style=style),\n",
    "    l1=ToggleButtons(options=[\"-0.1\", \"0\", \"0.0001\", \"0.0005\", \"0.001\", \"0.005\", \"0.01\", \"0.05\", \"0.1\"], \n",
    "                               value=\"0.0001\", description='Регуляризация L1: ', style=style),\n",
    "    l2=ToggleButtons(options=[\"-0.1\", \"0\", \"0.0001\", \"0.0005\", \"0.001\", \"0.005\", \"0.01\", \"0.05\", \"0.1\"], \n",
    "                               value=\"0.0001\", description='Регуляризация L2: ', style=style),\n",
    "    epochs=BoundedIntText(value=20, min=1, max=1000, step=1, description='Количество эпох: ', style=style),\n",
    "    create_new_model=Checkbox(value=True, description='Создать новую модель: ', style=style)\n",
    ")\n",
    "def interactive_learning(n_layers, loss_func, lr, l1, l2, \n",
    "                         layer_out_type,\n",
    "                         layer1_size, layer2_size, layer3_size, inner_layers_type,\n",
    "                         create_new_model, epochs):\n",
    "    layer_sizes = [layer1_size, layer2_size, layer3_size]\n",
    "    lr, l1, l2 = float(lr), float(l1), float(l2)\n",
    "    global model, history\n",
    "    \n",
    "    if create_new_model:\n",
    "        model = Sequential()\n",
    "        history = {'acc': [], 'val_acc': [], 'loss': [], 'val_loss': []}\n",
    "\n",
    "        if n_layers == 0:\n",
    "            model.add(Dense(len(y_train[0]), activation=layer_out_type, \n",
    "                            input_shape=x_train[0].shape, kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "        else:\n",
    "            model.add(Dense(layer_sizes[0], activation=inner_layers_type, \n",
    "                            input_shape=x_train[0].shape, kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "            for i in range(1, n_layers):\n",
    "                model.add(Dense(layer_sizes[i], activation=inner_layers_type,\n",
    "                               kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "            model.add(Dense(len(y_train[0]), activation=layer_out_type, kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "\n",
    "        model.compile(\n",
    "         optimizer = optimizers.sgd(lr=lr),\n",
    "         loss = loss_func,\n",
    "         metrics = [\"accuracy\"]\n",
    "        )\n",
    "    \n",
    "    for key, entry in model.fit(\n",
    "     x_train, y_train,\n",
    "     epochs=epochs,\n",
    "     validation_data=(x_test, y_test),\n",
    "     verbose=1\n",
    "    ).history.items():\n",
    "        history[key].extend(entry)\n",
    "    \n",
    "    print('History: ', history)\n",
    "    print('Accuracy: ', history['val_acc'][-1])\n",
    "    plot_accuracy(history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ai_dup]",
   "language": "python",
   "name": "conda-env-ai_dup-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
